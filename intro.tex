\section{Introduction}
% what is automl
Machine learning development often involves an iterative, trial-and-error process of selecting the appropriate set of models, parameters, and settings that is optimal for a target application. This is a time-consuming and cubersome process that require users to experiment with a large set of modelling decisions and empirically evaluate the performance of the resulting model. To address these issues, in the past several years, we have seen a growing number of \emph{\automl systems} that aims to support and automate various aspects of the machine learning process. \automl systems automatically search through a large space of possible modeling decisions, while optimizing for a set of desired objectives. \automl systems leverages techniques from the field of meta-learning (learning-to-learn), such as bayesian optimization and neural architecture search, that enable efficient tuning and selection across a large number of model and parameters. The goals of these system is to abstract away the manual search and experimentation process to the machines so that users can focus on other parts of the machine learning workflow, such as engaging with business stakeholders or validating the robustness of the model before deployment.
% Automated Machine Learning (auto-ml) techniques have been recently introduced to simplify and accelerate the process of developing machine learning models by automating tasks such as data cleaning, model selection, and hyperparameter tuning. The promise of auto-ml tools is to make machine learning more accessible for citizen data analysts by providing off-the-shelf solution for users with less technical background about machine learning. Auto-ml tools also promise machine learning experts the relief from the mechanical tasks such as hyperparameters tuning. 
In this study, we aim to examine where Auto-ml fits within a typical machine learning workflow, the perception of auto-ml from users of auto-ml, and auto-ml practitionersâ€™ usage behavior. 

% We ask 
% \begin{itemize}
%     \item RQ1: Who are the auto-ml users and non-auto-ml users? 
%     \item RQ2: How do auto-ml users perceive of auto-ml?
%     \item RQ3: What factors lead one to choose manual ML over auto-ml?
%     \item RQ4: What improvements in auto-ml would auto-ml users like to see? 
%     \item RQ5: What is the impact of auto-ml on AI explanability? Does auto-ml make ML abstruse for the interpretation of clients of data science teams?
%     \item RQ6: What is the ideal integration between human and Automated Machine Learning? 
% \end{itemize}

We contribute to this research space by engaging with users of Auto-ml to inform the design of systems for individuals who can most benefit from Auto-ml. In addition, we fill in the gap in the understanding of how auto-ml can be designed for data scientists to trust it enough to use it in practice by providing empirical evidence of perception of explainability, transparency and trust in auto-ml.

This is the first in-depth study of auto-ml practitioner to uncover their latent needs and wants. We reject the assumption that auto-ml design is to automate human workers, but rather to enhance humans' work. Here we extend prior research in calling for collaboration of human and AI and for AI to take on more an "augmentor" role rather than an "automator" role. The insights from the study will help guide the design of human-centered auto-ml systems. 


% In particular, we include in our study samples citizen data scientists, data scientist novices who are auto-ml users because their perspectives are lacking in the academic literature, despite being a major target group of auto-ml that promises to democratizes data science. 